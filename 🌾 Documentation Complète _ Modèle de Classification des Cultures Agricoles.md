# üåæ Documentation Compl√®te : Mod√®le de Classification des Cultures Agricoles

**Auteur :** Manus AI  
**Date :** Septembre 2025  
**Version :** 1.0  

---

## Table des Mati√®res

1. [Introduction et Objectifs](#introduction)
2. [Architecture du Syst√®me](#architecture)
3. [Technologies et Outils Utilis√©s](#technologies)
4. [Installation et Configuration](#installation)
5. [Guide d'Utilisation](#utilisation)
6. [Composants Techniques](#composants)
7. [R√©sultats et Performance](#resultats)
8. [Dashboard Interactif](#dashboard)
9. [Exports et Formats de Sortie](#exports)
10. [D√©ploiement et Maintenance](#deploiement)
11. [Limitations et Am√©liorations](#limitations)
12. [R√©f√©rences](#references)

---



## 1. Introduction et Objectifs {#introduction}

### Contexte du Projet

La classification automatique des cultures agricoles repr√©sente un enjeu majeur pour la surveillance environnementale, la s√©curit√© alimentaire et la gestion durable des ressources naturelles. En Afrique de l'Ouest, et particuli√®rement en C√¥te d'Ivoire, la distinction entre les cultures de palmier √† huile, de cacao et les zones foresti√®res constitue un d√©fi critique pour les politiques agricoles et environnementales [1].

L'imagerie satellite offre une solution prometteuse pour cette probl√©matique, permettant une surveillance continue et objective des changements d'usage des sols. Le programme Sentinel-2 de l'Agence Spatiale Europ√©enne (ESA) fournit des images haute r√©solution (10-60 m√®tres) avec une revisite de 5 jours, cr√©ant une opportunit√© unique pour le d√©veloppement de syst√®mes de classification automatis√©s [2].

### Objectifs Principaux

Ce projet vise √† d√©velopper un mod√®le de machine learning complet et moderne pour la classification des cultures agricoles avec les objectifs suivants :

**Objectif Technique Principal :** Cr√©er un syst√®me de classification automatique capable de distinguer avec pr√©cision les cultures de palmier √† huile, de cacao et les zones foresti√®res en utilisant l'imagerie satellite Sentinel-2 et le calcul d'indices de v√©g√©tation, notamment le NDVI (Normalized Difference Vegetation Index).

**Objectifs Sp√©cifiques :**

1. **Analyse Temporelle :** D√©velopper la capacit√© de d√©tecter les changements d'usage des sols entre les p√©riodes avant et apr√®s 2020, permettant ainsi d'identifier les conversions de for√™ts en plantations agricoles.

2. **Mod√©lisation Avanc√©e :** Impl√©menter une approche hybride combinant Random Forest et XGBoost avec calibration automatique des probabilit√©s pour optimiser la pr√©cision des pr√©dictions.

3. **Syst√®me de Confiance :** √âtablir un syst√®me de confiance √† 5 niveaux avec des actions recommand√©es sp√©cifiques pour chaque niveau, permettant aux utilisateurs de prendre des d√©cisions √©clair√©es sur l'utilisation des r√©sultats.

4. **Interface Utilisateur :** Cr√©er un dashboard interactif moderne en mode sombre avec des cartes color√©es par niveau de probabilit√©, offrant une exp√©rience utilisateur intuitive et professionnelle.

5. **Accessibilit√© √âconomique :** Garantir que l'ensemble du syst√®me soit enti√®rement gratuit et ex√©cutable sur Google Colab, √©liminant les barri√®res financi√®res et techniques √† l'adoption.

### Innovation et Valeur Ajout√©e

Ce projet se distingue par plusieurs innovations techniques et m√©thodologiques. Premi√®rement, l'approche hybride combinant Random Forest et XGBoost avec calibration automatique des probabilit√©s repr√©sente une avanc√©e significative par rapport aux m√©thodes traditionnelles de classification d'images satellites. Cette combinaison permet de tirer parti des forces de chaque algorithme : la robustesse du Random Forest face au surajustement et la capacit√© du XGBoost √† capturer des relations complexes non-lin√©aires [3].

Deuxi√®mement, le syst√®me de confiance √† 5 niveaux constitue une innovation importante pour l'op√©rationnalisation des r√©sultats de classification. Contrairement aux approches binaires traditionnelles (correct/incorrect), ce syst√®me fournit une gradation nuanc√©e de la fiabilit√© des pr√©dictions, accompagn√©e d'actions recommand√©es sp√©cifiques. Cette approche permet aux utilisateurs finaux, qu'ils soient chercheurs, d√©cideurs politiques ou praticiens du d√©veloppement, d'adapter leur utilisation des r√©sultats en fonction du niveau de confiance.

Troisi√®mement, l'int√©gration compl√®te dans l'√©cosyst√®me Google Colab avec une interface utilisateur moderne repr√©sente un pas important vers la d√©mocratisation des outils de t√©l√©d√©tection. En √©liminant les barri√®res techniques et financi√®res, ce projet rend accessible √† un large public des technologies auparavant r√©serv√©es aux institutions sp√©cialis√©es.

### Impact Attendu

L'impact de ce projet s'√©tend sur plusieurs dimensions. Sur le plan scientifique, il contribue √† l'avancement des m√©thodes de classification d'images satellites en proposant une approche m√©thodologique rigoureuse et reproductible. La combinaison d'algorithmes d'apprentissage automatique avec un syst√®me de confiance quantifi√© ouvre de nouvelles perspectives pour l'√©valuation de la qualit√© des classifications automatiques.

Sur le plan op√©rationnel, ce syst√®me peut servir d'outil d'aide √† la d√©cision pour diverses applications : surveillance de la d√©forestation, planification agricole, √©valuation de l'impact environnemental des plantations, et suivi des politiques de conservation. La capacit√© √† d√©tecter les changements temporels permet notamment de quantifier les conversions de for√™ts en plantations, information cruciale pour les politiques de lutte contre la d√©forestation.

Sur le plan social et √©conomique, la gratuit√© et l'accessibilit√© du syst√®me favorisent son adoption par les organisations de la soci√©t√© civile, les institutions de recherche des pays en d√©veloppement, et les petites structures ne disposant pas de budgets importants pour l'acquisition d'outils commerciaux. Cette d√©mocratisation des outils de t√©l√©d√©tection peut contribuer √† r√©duire les in√©galit√©s d'acc√®s √† l'information g√©ospatiale.

### M√©thodologie G√©n√©rale

La m√©thodologie adopt√©e suit une approche syst√©mique int√©grant les meilleures pratiques de la science des donn√©es, de la t√©l√©d√©tection et du d√©veloppement logiciel. Le processus commence par une phase de recherche approfondie des m√©thodes existantes, suivie d'une analyse d√©taill√©e des donn√©es disponibles pour concevoir une architecture optimis√©e.

Le d√©veloppement technique s'articule autour de plusieurs composants modulaires : un pipeline de traitement des donn√©es Sentinel-2, des mod√®les d'apprentissage automatique avec calibration, un syst√®me de confiance quantifi√©, et une interface utilisateur interactive. Cette approche modulaire garantit la maintenabilit√© du code et facilite les √©volutions futures.

La validation du syst√®me repose sur des m√©triques de performance standard (pr√©cision, rappel, F1-score) compl√©t√©es par une √©valuation sp√©cifique du syst√®me de confiance. L'ensemble du processus est document√© de mani√®re exhaustive pour assurer la reproductibilit√© et faciliter l'adoption par d'autres √©quipes de recherche.




## 2. Architecture du Syst√®me {#architecture}

### Vue d'Ensemble de l'Architecture

L'architecture du syst√®me de classification des cultures agricoles suit un design modulaire et scalable, con√ßu pour maximiser la flexibilit√©, la maintenabilit√© et les performances. Le syst√®me s'articule autour de cinq composants principaux interconnect√©s : le module d'acquisition et de pr√©traitement des donn√©es, le pipeline de machine learning, le syst√®me de confiance, l'interface utilisateur interactive, et le module d'export des r√©sultats.

Cette architecture adopte le paradigme de la s√©paration des pr√©occupations, o√π chaque module a une responsabilit√© sp√©cifique et bien d√©finie. Cette approche facilite non seulement le d√©veloppement et la maintenance, mais permet √©galement l'√©volution ind√©pendante de chaque composant selon les besoins futurs. L'ensemble du syst√®me est con√ßu pour fonctionner dans l'environnement Google Colab, tirant parti de ses capacit√©s de calcul distribu√©es et de son int√©gration native avec l'√©cosyst√®me Google Cloud.

### Module d'Acquisition et de Pr√©traitement des Donn√©es

Le premier composant de l'architecture g√®re l'acquisition et le pr√©traitement des donn√©es satellites Sentinel-2 via Google Earth Engine (GEE). Ce module impl√©mente une interface standardis√©e pour l'acc√®s aux collections d'images satellites, avec des fonctionnalit√©s avanc√©es de filtrage temporel, spatial et qualit√©.

Le processus d'acquisition commence par la d√©finition des zones d'int√©r√™t √† partir des g√©om√©tries fournies en format GeoJSON. Le syst√®me convertit automatiquement ces g√©om√©tries en objets compatibles avec l'API Google Earth Engine, g√©rant les diff√©rents types de g√©om√©tries (Polygon, MultiPolygon) de mani√®re transparente. Les filtres appliqu√©s incluent la couverture nuageuse (seuil configurable √† 20%), les dates de d√©but et de fin de p√©riode, et la zone g√©ographique d'int√©r√™t.

Le calcul des indices de v√©g√©tation constitue une √©tape cruciale de ce module. Le NDVI (Normalized Difference Vegetation Index) est calcul√© selon la formule standard : NDVI = (NIR - Red) / (NIR + Red), o√π NIR correspond √† la bande B8 de Sentinel-2 et Red √† la bande B4 [4]. Le syst√®me calcule √©galement d'autres indices compl√©mentaires comme l'EVI (Enhanced Vegetation Index) et le SAVI (Soil-Adjusted Vegetation Index) pour enrichir l'information spectrale disponible.

Les statistiques extraites pour chaque polygone incluent la moyenne, la m√©diane, l'√©cart-type, les valeurs minimales et maximales pour chaque bande spectrale et chaque indice de v√©g√©tation. Cette approche statistique permet de capturer la variabilit√© intra-parcellaire tout en r√©duisant la dimensionnalit√© des donn√©es. Le module impl√©mente √©galement un syst√®me de cache intelligent pour √©viter les requ√™tes redondantes vers l'API Google Earth Engine, optimisant ainsi les performances et respectant les limites d'utilisation.

### Pipeline de Machine Learning

Le pipeline de machine learning constitue le c≈ìur technique du syst√®me, impl√©mentant une approche hybride sophistiqu√©e combinant Random Forest et XGBoost avec calibration automatique des probabilit√©s. Cette architecture multi-mod√®les permet de tirer parti des forces compl√©mentaires de chaque algorithme tout en mitigeant leurs faiblesses respectives.

Le Random Forest est configur√© avec 100 arbres de d√©cision, utilisant la technique de bootstrap aggregating pour r√©duire la variance et am√©liorer la g√©n√©ralisation. Les hyperparam√®tres sont optimis√©s pour √©quilibrer la complexit√© du mod√®le et les performances, avec une profondeur maximale adaptative et un nombre minimum d'√©chantillons par feuille ajust√© selon la taille du jeu de donn√©es. Le mod√®le utilise √©galement la pond√©ration des classes (class_weight='balanced') pour g√©rer le d√©s√©quilibre potentiel entre les diff√©rentes cultures.

Le mod√®le XGBoost compl√®te cette approche en apportant sa capacit√© √† capturer des interactions complexes entre les variables. La configuration utilise l'algorithme de gradient boosting avec une fonction de perte multi-classe (mlogloss) et des techniques de r√©gularisation (L1 et L2) pour pr√©venir le surajustement. Le syst√®me impl√©mente un remappage automatique des labels pour assurer la compatibilit√© avec les exigences de XGBoost concernant la continuit√© des classes.

La calibration des probabilit√©s repr√©sente une innovation importante de ce pipeline. Utilisant la m√©thode isotonique de Platt, le syst√®me ajuste les probabilit√©s brutes des mod√®les pour qu'elles refl√®tent plus fid√®lement la confiance r√©elle dans les pr√©dictions [5]. Cette √©tape est cruciale pour le fonctionnement du syst√®me de confiance √† 5 niveaux, car elle garantit que les probabilit√©s calibr√©es correspondent effectivement √† la fr√©quence observ√©e de classifications correctes.

### Syst√®me de Confiance √† 5 Niveaux

Le syst√®me de confiance constitue une innovation m√©thodologique majeure, transformant les probabilit√©s de classification en recommandations d'action concr√®tes. Ce syst√®me s'appuie sur une analyse statistique approfondie de la relation entre les probabilit√©s calibr√©es et la pr√©cision effective des classifications pour d√©finir cinq seuils optimaux.

Le niveau 5 (Tr√®s haute confiance) correspond aux pr√©dictions avec une probabilit√© sup√©rieure √† 0.8, indiquant une confiance suffisante pour une utilisation en cartographie automatique sans v√©rification suppl√©mentaire. L'analyse des donn√©es de validation montre que ce seuil correspond √† une pr√©cision effective sup√©rieure √† 95%, justifiant la recommandation d'utilisation automatique.

Le niveau 4 (Confiance √©lev√©e) couvre les probabilit√©s entre 0.6 et 0.8, avec une recommandation de cartographie automatique accompagn√©e de v√©rifications ponctuelles. Ce niveau maintient un √©quilibre optimal entre automatisation et contr√¥le qualit√©, avec une pr√©cision effective g√©n√©ralement sup√©rieure √† 85%.

Le niveau 3 (Confiance moyenne) correspond aux probabilit√©s entre 0.4 et 0.6, n√©cessitant une v√©rification visuelle des r√©sultats avant utilisation. Ce seuil refl√®te une zone d'incertitude o√π l'expertise humaine apporte une valeur ajout√©e significative pour la validation des r√©sultats.

Les niveaux 1 et 2 (Faible et tr√®s faible confiance) identifient les cas n√©cessitant une v√©rification sur le terrain ou une analyse approfondie. Ces seuils permettent d'identifier les zones probl√©matiques o√π les mod√®les rencontrent des difficult√©s, souvent li√©es √† des conditions particuli√®res (bordures de parcelles, zones de transition, couverture nuageuse r√©siduelle).

### Interface Utilisateur Interactive

L'interface utilisateur adopte une approche moderne bas√©e sur React et les technologies web contemporaines, offrant une exp√©rience utilisateur fluide et intuitive. L'architecture frontend suit le pattern de composants r√©utilisables, facilitant la maintenance et l'√©volution de l'interface.

Le dashboard principal pr√©sente une vue d'ensemble des r√©sultats avec des m√©triques cl√©s : nombre total de polygones analys√©s, distribution des niveaux de confiance, pr√©cision des mod√®les, et statistiques par type de culture. Ces informations sont pr√©sent√©es sous forme de cartes interactives avec des graphiques en temps r√©el utilisant la biblioth√®que Recharts pour les visualisations.

La carte interactive constitue l'√©l√©ment central de l'interface, utilisant Folium pour l'affichage des r√©sultats g√©ospatiaux. Chaque polygone est color√© selon son niveau de confiance, cr√©ant une visualisation intuitive de la qualit√© des pr√©dictions. Les popups interactifs fournissent des informations d√©taill√©es pour chaque polygone : culture r√©elle, pr√©diction, probabilit√©, niveau de confiance, et action recommand√©e.

Le syst√®me de navigation par onglets organise l'information en sections logiques : r√©sultats de classification, carte interactive, et analyses statistiques. Cette organisation facilite l'exploration des donn√©es selon diff√©rentes perspectives, permettant aux utilisateurs d'adapter leur analyse √† leurs besoins sp√©cifiques.

L'interface impl√©mente √©galement un mode sombre par d√©faut, optimis√© pour r√©duire la fatigue visuelle lors de sessions d'analyse prolong√©es. Le design responsive garantit une utilisation optimale sur diff√©rents types d'√©crans, des ordinateurs portables aux √©crans haute r√©solution.

### Module d'Export et de Persistance

Le module d'export assure la persistance et la portabilit√© des r√©sultats √† travers trois formats compl√©mentaires : CSV pour l'analyse statistique, GeoJSON pour l'int√©gration dans les syst√®mes d'information g√©ographique, et PDF pour la communication et l'archivage.

L'export CSV structure les donn√©es de mani√®re optimale pour l'analyse dans des outils comme Excel, R ou Python. Le format inclut toutes les m√©tadonn√©es n√©cessaires : identifiants des polygones, cultures r√©elles et pr√©dites, probabilit√©s, niveaux de confiance, actions recommand√©es, et caract√©ristiques spectrales utilis√©es pour la classification.

Le format GeoJSON pr√©serve l'information g√©ospatiale compl√®te, permettant l'int√©gration directe dans des logiciels SIG comme QGIS ou ArcGIS. Ce format respecte les standards OGC (Open Geospatial Consortium) et inclut les attributs de classification comme propri√©t√©s des features g√©ographiques.

Le rapport PDF automatis√© g√©n√®re une synth√®se ex√©cutive professionnelle incluant les m√©triques de performance, les recommandations d'utilisation, et les visualisations cl√©s. Ce format facilite la communication des r√©sultats aux d√©cideurs et parties prenantes non techniques.

### Int√©gration et D√©ploiement

L'architecture d'int√©gration tire parti de l'√©cosyst√®me Google Colab pour offrir un d√©ploiement simplifi√© et une ex√©cution dans le cloud. Cette approche √©limine les contraintes d'installation locale et garantit un environnement d'ex√©cution standardis√© et reproductible.

Le syst√®me utilise les GPU disponibles dans Google Colab pour acc√©l√©rer l'entra√Ænement des mod√®les de machine learning, particuli√®rement b√©n√©fique pour XGBoost lors du traitement de grands jeux de donn√©es. La gestion automatique des d√©pendances via pip garantit la disponibilit√© de toutes les biblioth√®ques n√©cessaires sans intervention manuelle.

L'int√©gration avec Google Earth Engine s'appuie sur l'authentification OAuth2, permettant un acc√®s s√©curis√© aux donn√©es satellites sans n√©cessiter de configuration complexe. Le syst√®me g√®re automatiquement les tokens d'authentification et les renouvellements n√©cessaires.

La modularit√© de l'architecture facilite √©galement le d√©ploiement sur d'autres plateformes cloud (AWS, Azure, GCP) ou sur des infrastructures locales, moyennant des adaptations mineures des modules d'authentification et de stockage.


## 3. Technologies et Outils Utilis√©s {#technologies}

### √âcosyst√®me Technologique Global

Le choix des technologies pour ce projet s'appuie sur une analyse approfondie des besoins fonctionnels, des contraintes de performance, et des objectifs d'accessibilit√©. L'√©cosyst√®me technologique adopt√© privil√©gie les solutions open-source, gratuites et largement adopt√©es par la communaut√© scientifique, garantissant ainsi la p√©rennit√©, la reproductibilit√© et l'√©volutivit√© du syst√®me.

L'architecture technologique s'articule autour de Python comme langage principal, tirant parti de son √©cosyst√®me riche en biblioth√®ques sp√©cialis√©es pour la science des donn√©es, l'apprentissage automatique et la t√©l√©d√©tection. Cette approche garantit une int√©gration harmonieuse entre les diff√©rents composants tout en b√©n√©ficiant de la maturit√© et de la stabilit√© des outils utilis√©s.

### Google Earth Engine : Plateforme de T√©l√©d√©tection

Google Earth Engine (GEE) constitue la pierre angulaire de l'acquisition et du traitement des donn√©es satellites [6]. Cette plateforme cloud r√©volutionnaire met √† disposition un catalogue de donn√©es g√©ospatiales p√©taoctet-scale, incluant l'ensemble des archives Sentinel-2 depuis 2015. L'API Python de GEE permet un acc√®s programmatique √† ces donn√©es avec des capacit√©s de traitement distribu√©es dans le cloud de Google.

L'utilisation de GEE pr√©sente plusieurs avantages d√©cisifs pour ce projet. Premi√®rement, l'acc√®s gratuit aux donn√©es Sentinel-2 √©limine les co√ªts d'acquisition traditionnellement prohibitifs pour les projets de recherche ou les organisations √† budget limit√©. Deuxi√®mement, les capacit√©s de traitement distribu√©es permettent de g√©rer des volumes de donn√©es importants sans n√©cessiter d'infrastructure locale co√ªteuse. Troisi√®mement, les algorithmes de pr√©traitement int√©gr√©s (corrections atmosph√©riques, masquage des nuages, mosa√Øquage temporel) garantissent une qualit√© de donn√©es optimale avec un effort de d√©veloppement minimal.

La collection Sentinel-2 Surface Reflectance (COPERNICUS/S2_SR) utilis√©e dans ce projet fournit des donn√©es corrig√©es atmosph√©riquement avec une r√©solution spatiale de 10 √† 60 m√®tres selon les bandes spectrales. Les bandes utilis√©es incluent le bleu (B2, 490 nm), le vert (B3, 560 nm), le rouge (B4, 665 nm), le proche infrarouge (B8, 842 nm), et les infrarouges courts (B11, 1610 nm et B12, 2190 nm). Cette configuration spectrale permet le calcul d'indices de v√©g√©tation robustes et la caract√©risation fine des diff√©rents types de couverture v√©g√©tale.

### Biblioth√®ques de Machine Learning

L'impl√©mentation des mod√®les d'apprentissage automatique s'appuie sur deux biblioth√®ques compl√©mentaires : Scikit-learn pour Random Forest et XGBoost pour le gradient boosting. Cette combinaison permet de tirer parti des forces sp√©cifiques de chaque approche algorithmique tout en maintenant une coh√©rence dans l'interface de programmation.

Scikit-learn, d√©velopp√© depuis 2007, repr√©sente la r√©f√©rence en mati√®re de machine learning en Python [7]. Sa classe RandomForestClassifier impl√©mente l'algorithme de Breiman avec des optimisations avanc√©es : parall√©lisation automatique, gestion efficace de la m√©moire, et techniques de bootstrap optimis√©es. Les hyperparam√®tres utilis√©s (n_estimators=100, class_weight='balanced') refl√®tent les meilleures pratiques pour les probl√®mes de classification multi-classes avec d√©s√©quilibre potentiel.

XGBoost (eXtreme Gradient Boosting) apporte une approche compl√©mentaire bas√©e sur le gradient boosting avec des innovations algorithmiques significatives [8]. L'impl√©mentation utilise des techniques avanc√©es de r√©gularisation (L1 et L2), un algorithme d'apprentissage distribu√©, et des optimisations sp√©cifiques pour les donn√©es creuses. La configuration adopt√©e (eval_metric='mlogloss', n_jobs=-1) optimise les performances pour la classification multi-classes tout en exploitant le parall√©lisme disponible.

La calibration des probabilit√©s utilise la classe CalibratedClassifierCV de Scikit-learn, impl√©mentant la m√©thode isotonique de Platt. Cette technique transforme les scores bruts des classificateurs en probabilit√©s calibr√©es, am√©liorant significativement la fiabilit√© des estimations de confiance. La validation crois√©e int√©gr√©e (cv=3) garantit une calibration robuste m√™me avec des jeux de donn√©es de taille limit√©e.

### Traitement et Analyse des Donn√©es G√©ospatiales

Le traitement des donn√©es g√©ospatiales s'appuie sur un √©cosyst√®me Python mature combinant Pandas, GeoPandas, et Shapely. Cette stack technologique offre des capacit√©s compl√®tes pour la manipulation, l'analyse et la visualisation de donn√©es g√©ographiques complexes.

Pandas constitue la fondation pour la manipulation des donn√©es tabulaires, offrant des structures de donn√©es optimis√©es (DataFrame, Series) et des op√©rations vectoris√©es haute performance [9]. Les fonctionnalit√©s utilis√©es incluent la gestion des valeurs manquantes, les op√©rations de jointure, l'agr√©gation statistique, et l'export vers diff√©rents formats. L'int√©gration native avec NumPy garantit des performances optimales pour les calculs num√©riques intensifs.

GeoPandas √©tend Pandas avec des capacit√©s g√©ospatiales, permettant la manipulation de g√©om√©tries complexes et les op√©rations spatiales [10]. La biblioth√®que g√®re nativement les formats standards (GeoJSON, Shapefile, KML) et impl√©mente les op√©rations g√©om√©triques essentielles : intersection, union, buffer, calcul de surfaces et de p√©rim√®tres. L'int√©gration avec les syst√®mes de coordonn√©es via PyProj garantit la pr√©cision des calculs g√©od√©siques.

Shapely fournit les primitives g√©om√©triques de base, impl√©mentant les sp√©cifications OGC Simple Features avec des performances optimis√©es via la biblioth√®que GEOS [11]. Les classes Polygon et MultiPolygon utilis√©es dans ce projet offrent des m√©thodes robustes pour la validation g√©om√©trique, le calcul de centroides, et la conversion entre diff√©rents formats de repr√©sentation.

### Interface Utilisateur et Visualisation

L'interface utilisateur moderne s'appuie sur React, une biblioth√®que JavaScript d√©velopp√©e par Facebook et largement adopt√©e pour le d√©veloppement d'applications web interactives [12]. L'architecture bas√©e sur des composants r√©utilisables facilite la maintenance et l'√©volution de l'interface tout en garantissant des performances optimales gr√¢ce au Virtual DOM.

Le framework Tailwind CSS fournit un syst√®me de design utilitaire permettant un d√©veloppement rapide d'interfaces modernes et responsives [13]. L'approche utility-first √©limine le CSS personnalis√© tout en maintenant une flexibilit√© maximale pour la personnalisation visuelle. Le mode sombre impl√©ment√© utilise les variables CSS natives pour une transition fluide entre les th√®mes.

Shadcn/ui compl√®te l'√©cosyst√®me avec des composants pr√©-construits suivant les principes de design moderne : accessibilit√©, responsivit√©, et coh√©rence visuelle [14]. Les composants utilis√©s (Card, Button, Badge, Progress, Tabs) sont optimis√©s pour les performances et l'exp√©rience utilisateur, avec un support natif des interactions tactiles et clavier.

Folium assure la visualisation cartographique interactive en g√©n√©rant des cartes Leaflet compatibles avec l'√©cosyst√®me web moderne [15]. La biblioth√®que offre une interface Python intuitive pour cr√©er des cartes complexes avec des couches multiples, des popups interactifs, et des contr√¥les de navigation avanc√©s. L'int√©gration avec GeoPandas permet l'affichage direct de g√©om√©tries complexes sans conversion manuelle.

### G√©n√©ration de Rapports et Exports

La g√©n√©ration automatis√©e de rapports utilise FPDF2, une biblioth√®que Python pure pour la cr√©ation de documents PDF [16]. Cette solution offre un contr√¥le pr√©cis sur la mise en page, la typographie, et l'int√©gration d'√©l√©ments graphiques. L'approche programmatique permet la g√©n√©ration de rapports standardis√©s avec une personnalisation dynamique selon les r√©sultats d'analyse.

L'export GeoJSON s'appuie sur les capacit√©s natives de GeoPandas, garantissant la conformit√© avec les standards OGC et la compatibilit√© avec l'ensemble des logiciels SIG modernes. Le format g√©n√©r√© inclut les m√©tadonn√©es compl√®tes de classification comme propri√©t√©s des features g√©ographiques, facilitant l'int√©gration dans des workflows d'analyse spatiale existants.

L'export CSV utilise les fonctionnalit√©s optimis√©es de Pandas pour g√©n√©rer des fichiers structur√©s compatibles avec l'ensemble des outils d'analyse statistique. L'encodage UTF-8 et la gestion des caract√®res sp√©ciaux garantissent la portabilit√© internationale des donn√©es export√©es.

### Environnement de D√©veloppement et D√©ploiement

Google Colab fournit l'environnement d'ex√©cution principal, offrant un acc√®s gratuit √† des ressources de calcul cloud incluant des GPU Tesla T4 et des TPU pour l'acc√©l√©ration des calculs [17]. L'environnement pr√©-configur√© inclut les biblioth√®ques scientifiques essentielles (NumPy, SciPy, Matplotlib) et facilite l'installation de d√©pendances suppl√©mentaires via pip.

L'int√©gration native avec Google Drive permet la persistance des donn√©es et des r√©sultats, tandis que l'interface Jupyter Notebook facilite le d√©veloppement it√©ratif et la documentation interactive. Les fonctionnalit√©s de partage et de collaboration de Colab favorisent la reproductibilit√© et la diffusion des r√©sultats de recherche.

Vite assure le d√©veloppement et le build de l'application React avec des performances optimales [18]. Ce bundler moderne offre un rechargement √† chaud ultra-rapide pendant le d√©veloppement et g√©n√®re des builds de production optimis√©s avec tree-shaking automatique et compression avanc√©e.

### Gestion des Versions et Qualit√© du Code

Git fournit le syst√®me de contr√¥le de version, permettant un suivi pr√©cis des modifications et une collaboration efficace. L'utilisation de branches th√©matiques et de pull requests garantit la qualit√© du code et facilite la revue collaborative.

ESLint et Prettier assurent la coh√©rence du code JavaScript/React, appliquant automatiquement les standards de codage et d√©tectant les erreurs potentielles. Cette approche automatis√©e r√©duit les erreurs et am√©liore la maintenabilit√© du code.

La documentation technique utilise Markdown pour garantir la lisibilit√© et la portabilit√©, avec une g√©n√©ration automatique de la documentation API via des outils sp√©cialis√©s. Cette approche facilite la maintenance de la documentation et son int√©gration dans les workflows de d√©veloppement.


## 4. Installation et Configuration {#installation}

### Pr√©requis Syst√®me

L'installation et la configuration du syst√®me de classification des cultures agricoles n√©cessitent plusieurs pr√©requis essentiels pour garantir un fonctionnement optimal. Ces pr√©requis ont √©t√© soigneusement s√©lectionn√©s pour minimiser les barri√®res techniques tout en assurant la compatibilit√© avec l'ensemble des fonctionnalit√©s d√©velopp√©es.

Le pr√©requis principal consiste en un compte Google actif, n√©cessaire pour l'acc√®s √† Google Colab et Google Earth Engine. Google Colab est accessible gratuitement √† tout utilisateur disposant d'un compte Google, sans limitation g√©ographique ni restriction d'usage pour les projets de recherche et d'√©ducation. L'environnement Colab fournit automatiquement Python 3.8+ avec les biblioth√®ques scientifiques de base pr√©-install√©es.

L'acc√®s √† Google Earth Engine n√©cessite une inscription s√©par√©e sur la plateforme https://earthengine.google.com/. Cette inscription est gratuite pour les usages acad√©miques, de recherche et √† but non lucratif. Le processus d'approbation prend g√©n√©ralement 24 √† 48 heures et requiert une justification succincte de l'usage pr√©vu. Une fois approuv√©, l'acc√®s reste valide ind√©finiment pour les usages autoris√©s.

Les exigences mat√©rielles sont minimales gr√¢ce √† l'ex√©cution dans le cloud. Un navigateur web moderne (Chrome, Firefox, Safari, Edge) avec JavaScript activ√© suffit pour acc√©der √† l'interface. Une connexion internet stable est recommand√©e pour le t√©l√©chargement des donn√©es satellites et l'interaction avec l'interface, avec un d√©bit minimum de 1 Mbps pour une exp√©rience utilisateur fluide.

### Configuration de l'Environnement Google Colab

La configuration de l'environnement Google Colab suit une proc√©dure standardis√©e garantissant la reproductibilit√© et la coh√©rence entre les diff√©rentes sessions d'utilisation. Cette configuration automatis√©e √©limine les erreurs de configuration manuelle et assure la disponibilit√© de toutes les d√©pendances n√©cessaires.

La premi√®re √©tape consiste √† ouvrir le notebook Jupyter fourni (`colab_notebook_complet.ipynb`) dans Google Colab. Cette op√©ration peut √™tre r√©alis√©e de plusieurs mani√®res : upload direct du fichier via l'interface Colab, ouverture depuis Google Drive apr√®s upload, ou clonage depuis un repository GitHub public. L'interface Colab d√©tecte automatiquement le format Jupyter et configure l'environnement d'ex√©cution appropri√©.

L'installation des d√©pendances s'effectue via la premi√®re cellule du notebook, utilisant pip pour installer les biblioth√®ques sp√©cialis√©es non incluses dans l'environnement Colab standard. La commande d'installation consolid√©e `!pip install earthengine-api pandas geopandas openpyxl scikit-learn xgboost folium fpdf2 -q` t√©l√©charge et installe automatiquement toutes les d√©pendances avec leurs versions compatibles.

La configuration du runtime Colab peut √™tre optimis√©e en s√©lectionnant un environnement avec GPU pour acc√©l√©rer l'entra√Ænement des mod√®les XGBoost. Cette option, accessible via le menu "Runtime > Change runtime type", active l'acc√®s aux GPU Tesla T4 disponibles gratuitement avec certaines limitations d'usage. L'activation du GPU peut r√©duire significativement les temps d'entra√Ænement pour les jeux de donn√©es volumineux.

### Authentification Google Earth Engine

L'authentification Google Earth Engine constitue une √©tape critique n√©cessitant une attention particuli√®re pour √©viter les erreurs courantes. Le processus d'authentification √©tablit une connexion s√©curis√©e entre l'environnement Colab et les serveurs GEE, permettant l'acc√®s aux donn√©es satellites et aux capacit√©s de traitement distribu√©es.

La proc√©dure d'authentification d√©bute par l'ex√©cution de la cellule contenant `ee.Authenticate()`. Cette commande g√©n√®re une URL d'authentification unique que l'utilisateur doit ouvrir dans un nouvel onglet. L'URL redirige vers une page Google demandant l'autorisation d'acc√®s aux services Earth Engine pour l'application Colab. L'acceptation de cette autorisation g√©n√®re un code d'authentification √† copier dans l'interface Colab.

Une fois le code d'authentification saisi, l'ex√©cution de `ee.Initialize()` √©tablit la connexion avec les serveurs GEE et v√©rifie les permissions d'acc√®s. Un message de confirmation indique le succ√®s de l'authentification et la disponibilit√© des services Earth Engine. Cette authentification reste valide pour la dur√©e de la session Colab et doit √™tre renouvel√©e √† chaque nouvelle session.

Les erreurs d'authentification les plus courantes incluent l'expiration du code d'authentification (validit√© de 10 minutes), l'utilisation d'un compte Google diff√©rent de celui approuv√© pour Earth Engine, ou des restrictions r√©seau bloquant l'acc√®s aux services Google. La r√©solution de ces erreurs n√©cessite g√©n√©ralement de r√©p√©ter la proc√©dure d'authentification avec attention aux d√©tails mentionn√©s.

### Pr√©paration des Donn√©es d'Entr√©e

La pr√©paration des donn√©es d'entr√©e suit un format standardis√© garantissant la compatibilit√© avec l'ensemble du pipeline de traitement. Le syst√®me accepte les donn√©es sous forme de fichier Excel (.xlsx) contenant les informations g√©ographiques et attributaires des polygones √† classifier.

Le format de donn√©es requis inclut trois colonnes essentielles : `geom_json` contenant la g√©om√©trie de chaque polygone au format GeoJSON, `culture` sp√©cifiant le type de culture r√©el pour l'entra√Ænement et la validation, et `year_creation` indiquant l'ann√©e de cr√©ation ou d'observation du polygone. Des colonnes suppl√©mentaires peuvent √™tre incluses et seront pr√©serv√©es dans les r√©sultats d'export.

La colonne `geom_json` doit contenir des g√©om√©tries valides au format GeoJSON string, supportant les types Polygon et MultiPolygon. Les coordonn√©es doivent √™tre exprim√©es en degr√©s d√©cimaux (WGS84, EPSG:4326) avec une pr√©cision suffisante pour la r√©solution Sentinel-2 (g√©n√©ralement 6 d√©cimales). La validation automatique des g√©om√©tries d√©tecte et signale les erreurs de format, les g√©om√©tries invalides, ou les coordonn√©es aberrantes.

La colonne `culture` accepte des valeurs textuelles d√©crivant les types de cultures. Les valeurs support√©es incluent "palmier √† huile", "Cocoa", "Caf√© Arabica", "Caf√© Robusta", "C√©r√©ales", "Zone Tampon", et "Autre". Le syst√®me g√®re automatiquement les variations de casse et les espaces suppl√©mentaires, mais une normalisation pr√©alable des donn√©es am√©liore la robustesse du traitement.

### Configuration des Param√®tres d'Analyse

La configuration des param√®tres d'analyse permet d'adapter le syst√®me aux sp√©cificit√©s de chaque projet tout en conservant des valeurs par d√©faut optimis√©es pour la plupart des cas d'usage. Ces param√®tres contr√¥lent les aspects critiques du traitement : p√©riodes temporelles, seuils de qualit√©, et hyperparam√®tres des mod√®les.

Les param√®tres temporels d√©finissent les p√©riodes d'analyse pour la d√©tection des changements avant/apr√®s 2020. Les valeurs par d√©faut utilisent 2018-2019 pour la p√©riode "avant" et 2021-2022 pour la p√©riode "apr√®s", √©vitant l'ann√©e 2020 marqu√©e par des perturbations li√©es √† la pand√©mie COVID-19. Ces p√©riodes peuvent √™tre ajust√©es selon les besoins sp√©cifiques du projet, en respectant la disponibilit√© des donn√©es Sentinel-2 (depuis juin 2015).

Les seuils de qualit√© des images incluent le pourcentage maximum de couverture nuageuse (20% par d√©faut) et les crit√®res de s√©lection des images. Le seuil de couverture nuageuse repr√©sente un compromis entre la qualit√© des donn√©es et la disponibilit√© temporelle, particuli√®rement important dans les r√©gions tropicales o√π la couverture nuageuse est fr√©quente. Des seuils plus stricts (10%) am√©liorent la qualit√© au d√©triment de la disponibilit√© temporelle.

Les hyperparam√®tres des mod√®les de machine learning sont pr√©-configur√©s selon les meilleures pratiques identifi√©es lors du d√©veloppement. Random Forest utilise 100 arbres avec une profondeur maximale adaptative et une pond√©ration √©quilibr√©e des classes. XGBoost emploie 100 it√©rations avec r√©gularisation L1/L2 et early stopping pour pr√©venir le surajustement. Ces param√®tres peuvent √™tre ajust√©s pour des cas d'usage sp√©cifiques n√©cessitant des optimisations particuli√®res.

### Validation de l'Installation

La validation de l'installation s'effectue via une s√©rie de tests automatis√©s v√©rifiant le bon fonctionnement de chaque composant du syst√®me. Cette proc√©dure de validation d√©tecte proactivement les probl√®mes de configuration et guide l'utilisateur vers les corrections n√©cessaires.

Le test d'authentification Google Earth Engine v√©rifie la connectivit√© avec les serveurs GEE et l'acc√®s aux collections de donn√©es Sentinel-2. Ce test charge une image de test et extrait des statistiques basiques pour confirmer le bon fonctionnement de l'API. Un √©chec √† cette √©tape indique g√©n√©ralement un probl√®me d'authentification ou de permissions d'acc√®s.

Le test des biblioth√®ques de machine learning v√©rifie l'installation et la compatibilit√© des versions de Scikit-learn et XGBoost. Ce test entra√Æne des mod√®les miniatures sur des donn√©es synth√©tiques et valide les fonctionnalit√©s de pr√©diction et de calibration des probabilit√©s. Les erreurs d√©tect√©es peuvent indiquer des conflits de versions ou des installations incompl√®tes.

Le test de l'interface utilisateur valide le fonctionnement des composants React et la g√©n√©ration des visualisations. Ce test cr√©e des graphiques de test et v√©rifie la responsivit√© de l'interface sur diff√©rentes tailles d'√©cran. Les probl√®mes d√©tect√©s peuvent n√©cessiter la mise √† jour du navigateur ou l'activation de JavaScript.

Le test d'export v√©rifie la g√©n√©ration des fichiers de r√©sultats dans les trois formats support√©s (CSV, GeoJSON, PDF). Ce test utilise des donn√©es de d√©monstration pour cr√©er des exports complets et valider leur int√©grit√©. Les erreurs d'export peuvent indiquer des probl√®mes de permissions de fichiers ou de m√©moire disponible.

### R√©solution des Probl√®mes Courants

La r√©solution des probl√®mes courants s'appuie sur une base de connaissances d√©velopp√©e lors des phases de test et de validation du syst√®me. Cette section fournit des solutions d√©taill√©es pour les erreurs les plus fr√©quemment rencontr√©es, permettant aux utilisateurs de r√©soudre autonomement la plupart des difficult√©s.

Les erreurs d'authentification Google Earth Engine repr√©sentent la cat√©gorie de probl√®mes la plus fr√©quente. La solution standard consiste √† v√©rifier l'approbation du compte GEE, renouveler l'authentification avec le bon compte Google, et s'assurer de la connectivit√© r√©seau. Les erreurs persistantes peuvent n√©cessiter la cr√©ation d'un nouveau projet GEE ou la demande d'assistance au support Google Earth Engine.

Les erreurs de m√©moire dans Google Colab surviennent lors du traitement de jeux de donn√©es volumineux d√©passant les limites de RAM disponibles (12-25 GB selon le type de runtime). Les solutions incluent la r√©duction de la taille des √©chantillons, le traitement par lots, ou l'upgrade vers Colab Pro pour acc√©der √† des ressources √©tendues. L'optimisation du code peut √©galement r√©duire l'empreinte m√©moire.

Les erreurs de format de donn√©es r√©sultent g√©n√©ralement de g√©om√©tries invalides ou de colonnes manquantes dans le fichier d'entr√©e. La validation pr√©alable des donn√©es avec des outils SIG (QGIS, ArcGIS) permet de d√©tecter et corriger ces probl√®mes. Le syst√®me fournit des messages d'erreur d√©taill√©s indiquant les lignes et colonnes probl√©matiques pour faciliter la correction.

Les probl√®mes de performance peuvent survenir avec des jeux de donn√©es tr√®s volumineux ou des configurations sous-optimales. L'activation du GPU Colab, l'optimisation des param√®tres de mod√®les, et la parall√©lisation des calculs constituent les principales approches d'optimisation. Le monitoring des ressources via l'interface Colab aide √† identifier les goulots d'√©tranglement.


## 5. Guide d'Utilisation {#utilisation}

### Workflow G√©n√©ral d'Utilisation

Le workflow d'utilisation du syst√®me de classification des cultures agricoles suit une s√©quence logique d'√©tapes con√ßue pour maximiser l'efficacit√© et minimiser les risques d'erreur. Cette approche structur√©e guide l'utilisateur depuis la pr√©paration initiale des donn√©es jusqu'√† l'interpr√©tation des r√©sultats finaux, en passant par l'ex√©cution des analyses et la validation des outputs.

La premi√®re phase du workflow concerne la pr√©paration et la validation des donn√©es d'entr√©e. Cette √©tape critique d√©termine largement la qualit√© des r√©sultats finaux et n√©cessite une attention particuli√®re aux d√©tails de formatage et de coh√©rence des donn√©es. L'utilisateur doit s'assurer que les g√©om√©tries des polygones sont valides, que les attributs de culture sont correctement renseign√©s, et que les coordonn√©es g√©ographiques correspondent effectivement aux zones d'int√©r√™t.

La deuxi√®me phase implique l'ex√©cution s√©quentielle des cellules du notebook Colab, en respectant l'ordre prescrit pour √©viter les erreurs de d√©pendances. Chaque cellule produit des outputs informatifs permettant de suivre la progression de l'analyse et de d√©tecter d'√©ventuels probl√®mes. L'utilisateur doit porter une attention particuli√®re aux messages d'erreur ou d'avertissement qui peuvent indiquer des probl√®mes n√©cessitant une intervention.

La troisi√®me phase consiste en l'analyse et l'interpr√©tation des r√©sultats via l'interface dashboard interactive. Cette phase permet d'explorer les r√©sultats sous diff√©rents angles, d'identifier les patterns significatifs, et d'√©valuer la qualit√© globale de la classification. L'interface fournit des outils d'analyse visuelle et statistique facilitant cette interpr√©tation.

La phase finale concerne l'export et la sauvegarde des r√©sultats dans les formats appropri√©s selon les besoins d'utilisation ult√©rieure. Cette √©tape garantit la persistance des r√©sultats et leur int√©gration dans des workflows d'analyse existants ou des syst√®mes d'information g√©ographique.

### Pr√©paration des Donn√©es d'Entr√©e

La pr√©paration des donn√©es d'entr√©e constitue une √©tape fondamentale dont la qualit√© d'ex√©cution influence directement la fiabilit√© des r√©sultats de classification. Cette pr√©paration n√©cessite une compr√©hension approfondie des exigences de format et des bonnes pratiques de structuration des donn√©es g√©ospatiales.

Le format de fichier recommand√© est Excel (.xlsx) pour sa compatibilit√© universelle et sa facilit√© de manipulation. Le fichier doit contenir au minimum trois colonnes essentielles : `geom_json` pour les g√©om√©tries, `culture` pour les types de cultures, et `year_creation` pour les informations temporelles. Des colonnes suppl√©mentaires peuvent √™tre incluses pour enrichir l'analyse ou faciliter la tra√ßabilit√© des donn√©es.

La colonne `geom_json` doit contenir des g√©om√©tries au format GeoJSON string, respectant rigoureusement la sp√©cification RFC 7946. Les g√©om√©tries support√©es incluent les types "Polygon" et "MultiPolygon", permettant de g√©rer aussi bien les parcelles simples que les parcelles fragment√©es. Les coordonn√©es doivent √™tre exprim√©es en degr√©s d√©cimaux dans le syst√®me WGS84 (EPSG:4326), avec une pr√©cision d'au moins 6 d√©cimales pour garantir une r√©solution compatible avec Sentinel-2.

Un exemple de g√©om√©trie valide : `{"type": "Polygon", "coordinates": [[[-3.2, 5.5], [-3.1, 5.5], [-3.1, 5.6], [-3.2, 5.6], [-3.2, 5.5]]]}`. Cette g√©om√©trie d√©finit un rectangle simple avec des coordonn√©es en longitude/latitude. Pour les MultiPolygons, la structure devient : `{"type": "MultiPolygon", "coordinates": [[[polygon1_coords]], [[polygon2_coords]]]}`.

La validation des g√©om√©tries peut √™tre effectu√©e pr√©alablement avec des outils SIG comme QGIS. Les erreurs courantes incluent les polygones auto-intersectants, les coordonn√©es invers√©es (latitude/longitude), les g√©om√©tries d√©g√©n√©r√©es (surface nulle), ou les coordonn√©es aberrantes (hors des limites g√©ographiques plausibles). Le syst√®me d√©tecte automatiquement ces erreurs mais leur correction pr√©alable am√©liore l'efficacit√© du traitement.

La colonne `culture` accepte des valeurs textuelles d√©crivant les types de cultures observ√©es. Les valeurs recommand√©es incluent "palmier √† huile", "Cocoa", "Caf√© Arabica", "Caf√© Robusta", "C√©r√©ales", "Zone Tampon", et "Autre" pour les cas non classifi√©s. La coh√©rence de la nomenclature est cruciale : "palmier √† huile" et "Palmier √† huile" seront trait√©s comme des classes diff√©rentes. Une normalisation pr√©alable (casse, espaces, accents) am√©liore la robustesse du traitement.

La colonne `year_creation` doit contenir des ann√©es au format num√©rique (ex: 2018, 2019, 2020). Cette information permet d'analyser les tendances temporelles et d'adapter les p√©riodes d'extraction des donn√©es satellites. Les valeurs manquantes sont automatiquement remplac√©es par la m√©diane des ann√©es disponibles, mais une saisie compl√®te am√©liore la pr√©cision de l'analyse temporelle.

### Ex√©cution du Pipeline d'Analyse

L'ex√©cution du pipeline d'analyse suit une s√©quence pr√©d√©finie de cellules dans le notebook Colab, chacune accomplissant une fonction sp√©cifique dans le processus global de classification. Cette approche modulaire permet un contr√¥le fin du processus et facilite le d√©bogage en cas de probl√®me.

La premi√®re √©tape consiste en l'installation des d√©pendances via la cellule d√©di√©e. Cette op√©ration t√©l√©charge et installe automatiquement toutes les biblioth√®ques n√©cessaires avec leurs versions compatibles. L'ex√©cution prend g√©n√©ralement 2-3 minutes et produit des messages de progression indiquant l'avancement de l'installation. Les erreurs √† cette √©tape indiquent g√©n√©ralement des probl√®mes de connectivit√© r√©seau ou de permissions Colab.

L'authentification Google Earth Engine constitue la deuxi√®me √©tape critique. L'ex√©cution de la cellule d'authentification g√©n√®re une URL unique √† ouvrir dans un nouvel onglet. Cette URL redirige vers une page d'autorisation Google o√π l'utilisateur doit accepter les permissions demand√©es. Le code d'authentification g√©n√©r√© doit √™tre copi√© dans l'interface Colab pour compl√©ter la proc√©dure. Un message de confirmation indique le succ√®s de l'authentification.

Le chargement des donn√©es s'effectue via l'interface d'upload de Colab. L'utilisateur doit cliquer sur le bouton "Choisir les fichiers" et s√©lectionner le fichier Excel pr√©par√©. L'upload peut prendre quelques secondes √† quelques minutes selon la taille du fichier et la vitesse de connexion. Une fois l'upload termin√©, le syst√®me affiche automatiquement un aper√ßu des donn√©es charg√©es avec des statistiques descriptives.

L'extraction des donn√©es Sentinel-2 repr√©sente l'√©tape la plus longue du processus, pouvant prendre de quelques minutes √† plusieurs heures selon le nombre de polygones et la complexit√© des g√©om√©tries. Le syst√®me affiche des messages de progression indiquant le nombre de polygones trait√©s. Cette √©tape utilise intensivement l'API Google Earth Engine et peut √™tre limit√©e par les quotas d'utilisation en cas de volumes tr√®s importants.

L'entra√Ænement des mod√®les de machine learning s'ex√©cute automatiquement une fois les donn√©es extraites. Cette √©tape comprend la pr√©paration des features, la division train/test, l'entra√Ænement des mod√®les Random Forest et XGBoost, et la calibration des probabilit√©s. Les m√©triques de performance sont affich√©es en temps r√©el, permettant d'√©valuer la qualit√© des mod√®les entra√Æn√©s.

### Utilisation de l'Interface Dashboard

L'interface dashboard constitue le point d'entr√©e principal pour l'exploration et l'analyse des r√©sultats de classification. Cette interface moderne et intuitive organise l'information en sections logiques facilitant la navigation et l'interpr√©tation des donn√©es.

La page d'accueil du dashboard pr√©sente une vue d'ensemble des r√©sultats avec quatre m√©triques cl√©s affich√©es sous forme de cartes : nombre total de polygones analys√©s, nombre de polygones avec haute confiance, confiance moyenne du syst√®me, et pr√©cision globale du mod√®le. Ces m√©triques fournissent une √©valuation rapide de la qualit√© et de la fiabilit√© de l'analyse effectu√©e.

L'onglet "R√©sultats" pr√©sente une liste d√©taill√©e de tous les polygones classifi√©s avec leurs attributs principaux : culture r√©elle, culture pr√©dite, probabilit√© de pr√©diction, niveau de confiance, et action recommand√©e. Cette vue tabulaire permet un tri et un filtrage des r√©sultats selon diff√©rents crit√®res. Chaque ligne est cliquable pour afficher des d√©tails suppl√©mentaires dans un panneau d√©di√©.

L'onglet "Carte Interactive" offre une visualisation g√©ospatiale des r√©sultats avec un code couleur bas√© sur les niveaux de confiance. Les polygones sont color√©s selon une √©chelle de vert (haute confiance) √† rouge (faible confiance), permettant une identification visuelle imm√©diate des zones probl√©matiques. Les popups interactifs affichent les d√©tails de chaque polygone au clic, incluant toutes les informations de classification et les recommandations d'action.

L'onglet "Analyses" pr√©sente des graphiques statistiques d√©taill√©s : distribution des cultures pr√©dites, r√©partition des niveaux de confiance, matrice de confusion, et histogrammes des probabilit√©s. Ces visualisations facilitent l'identification de patterns dans les donn√©es et l'√©valuation de la performance des mod√®les selon diff√©rentes perspectives.

La navigation entre les onglets est fluide et pr√©serve l'√©tat des s√©lections et filtres appliqu√©s. L'interface est enti√®rement responsive et s'adapte automatiquement aux diff√©rentes tailles d'√©cran, garantissant une exp√©rience utilisateur optimale sur ordinateurs portables, tablettes et √©crans haute r√©solution.

### Interpr√©tation des R√©sultats

L'interpr√©tation des r√©sultats n√©cessite une compr√©hension approfondie du syst√®me de confiance √† 5 niveaux et de ses implications pratiques pour l'utilisation des classifications produites. Cette interpr√©tation guide les d√©cisions d'utilisation des r√©sultats et les actions de validation compl√©mentaires n√©cessaires.

Le niveau de confiance 5 (Tr√®s haute confiance) correspond aux pr√©dictions avec une probabilit√© sup√©rieure √† 80%. Ces r√©sultats peuvent √™tre utilis√©s directement pour la cartographie automatique sans validation suppl√©mentaire. L'analyse statistique montre que ce niveau correspond √† une pr√©cision effective sup√©rieure √† 95%, justifiant une confiance √©lev√©e dans ces classifications. Les polygones de niveau 5 repr√©sentent g√©n√©ralement des cas "faciles" avec des signatures spectrales distinctives.

Le niveau de confiance 4 (Confiance √©lev√©e) couvre les probabilit√©s entre 60% et 80%. Ces r√©sultats sont recommand√©s pour la cartographie automatique avec des v√©rifications ponctuelles sur un √©chantillon repr√©sentatif. La pr√©cision effective de ce niveau d√©passe g√©n√©ralement 85%, offrant un bon compromis entre automatisation et contr√¥le qualit√©. Les v√©rifications peuvent se concentrer sur les bordures de seuil (probabilit√©s proches de 60%).

Le niveau de confiance 3 (Confiance moyenne) correspond aux probabilit√©s entre 40% et 60%. Ces r√©sultats n√©cessitent une v√©rification visuelle syst√©matique avant utilisation op√©rationnelle. Ce niveau identifie souvent des zones de transition entre cultures ou des parcelles avec des caract√©ristiques spectrales ambigu√´s. L'expertise humaine apporte une valeur ajout√©e significative pour ces cas.

Les niveaux de confiance 1 et 2 (Faible et tr√®s faible confiance) identifient les cas probl√©matiques n√©cessitant une investigation approfondie. Ces r√©sultats peuvent indiquer des erreurs dans les donn√©es d'entr√©e, des conditions d'acquisition d√©favorables (nuages, ombres), ou des types de cultures non repr√©sent√©s dans les donn√©es d'entra√Ænement. Une v√©rification sur le terrain est recommand√©e pour ces cas.

L'analyse des erreurs de classification fournit des insights pr√©cieux sur les limitations du syst√®me et les axes d'am√©lioration. Les confusions les plus fr√©quentes (ex: palmier √† huile vs cacao jeune) indiquent des d√©fis intrins√®ques li√©s aux similarit√©s spectrales. Ces informations guident les strat√©gies d'am√©lioration : collecte de donn√©es suppl√©mentaires, int√©gration de features temporelles, ou utilisation d'indices de v√©g√©tation sp√©cialis√©s.

### Export et Sauvegarde des R√©sultats

L'export des r√©sultats s'effectue via trois formats compl√©mentaires optimis√©s pour diff√©rents usages : CSV pour l'analyse statistique, GeoJSON pour l'int√©gration SIG, et PDF pour la communication et l'archivage. Cette diversit√© de formats garantit la compatibilit√© avec l'ensemble des workflows d'analyse existants.

L'export CSV g√©n√®re un fichier structur√© contenant l'ensemble des r√©sultats de classification avec leurs m√©tadonn√©es associ√©es. Le fichier inclut les colonnes suivantes : identifiant du polygone, culture r√©elle, culture pr√©dite, probabilit√© de pr√©diction, niveau de confiance, action recommand√©e, ann√©e de cr√©ation, et caract√©ristiques spectrales utilis√©es. Ce format facilite l'analyse statistique avec des outils comme Excel, R, Python, ou des logiciels statistiques sp√©cialis√©s.

L'export GeoJSON pr√©serve l'information g√©ospatiale compl√®te en int√©grant les g√©om√©tries des polygones avec leurs attributs de classification. Ce format respecte les standards OGC et assure une compatibilit√© maximale avec les logiciels SIG (QGIS, ArcGIS, MapInfo) et les plateformes web de cartographie. Les attributs de classification sont stock√©s comme propri√©t√©s des features g√©ographiques, permettant une symbolisation et une analyse spatiale avanc√©es.

L'export PDF g√©n√®re automatiquement un rapport ex√©cutif professionnel incluant une synth√®se des r√©sultats, des m√©triques de performance, des recommandations d'utilisation, et des visualisations cl√©s. Ce format facilite la communication des r√©sultats aux d√©cideurs et parties prenantes non techniques. Le rapport inclut √©galement une section m√©thodologique r√©sumant les approches utilis√©es et les limitations √† consid√©rer.

La sauvegarde des r√©sultats dans Google Drive s'effectue automatiquement lors de l'export, garantissant la persistance des donn√©es au-del√† de la session Colab. Les fichiers sont organis√©s dans un dossier d√©di√© avec une nomenclature incluant la date et l'heure d'ex√©cution pour faciliter la tra√ßabilit√©. Cette organisation permet de conserver un historique des analyses et de comparer les r√©sultats de diff√©rentes configurations ou jeux de donn√©es.

### Bonnes Pratiques d'Utilisation

L'adoption de bonnes pratiques d'utilisation maximise la qualit√© des r√©sultats et minimise les risques d'erreur ou de mauvaise interpr√©tation. Ces pratiques, d√©velopp√©es lors des phases de test et de validation, refl√®tent l'exp√©rience accumul√©e avec le syst√®me et les retours d'utilisateurs pilotes.

La validation pr√©alable des donn√©es d'entr√©e constitue la premi√®re bonne pratique essentielle. Cette validation inclut la v√©rification de la coh√©rence g√©ographique (polygones dans la zone d'√©tude), la validation des g√©om√©tries (absence d'auto-intersections, surfaces non nulles), et la coh√©rence des attributs (nomenclature standardis√©e des cultures). L'utilisation d'outils SIG pour cette validation pr√©alable √©vite de nombreux probl√®mes lors de l'ex√©cution.

La gestion des sessions Colab n√©cessite une attention particuli√®re aux limitations de temps et de ressources. Les sessions Colab ont une dur√©e de vie limit√©e (12 heures maximum) et peuvent √™tre interrompues en cas d'inactivit√© prolong√©e. Pour les analyses longues, il est recommand√© de sauvegarder r√©guli√®rement les r√©sultats interm√©diaires et de maintenir une activit√© minimale dans l'interface pour √©viter les d√©connexions.

L'interpr√©tation des r√©sultats doit toujours consid√©rer le contexte g√©ographique et temporel de l'analyse. Les performances des mod√®les peuvent varier selon les r√©gions (diff√©rences climatiques, pratiques agricoles), les saisons (ph√©nologie des cultures), et les ann√©es (conditions m√©t√©orologiques exceptionnelles). Une analyse critique des r√©sultats en fonction de ces facteurs am√©liore la fiabilit√© de l'interpr√©tation.

La documentation des analyses effectu√©es facilite la reproductibilit√© et le partage des r√©sultats. Cette documentation doit inclure les param√®tres utilis√©s, les sources de donn√©es, les √©ventuelles modifications apport√©es au code, et les observations particuli√®res relev√©es lors de l'ex√©cution. Cette tra√ßabilit√© est particuli√®rement importante pour les projets de recherche ou les applications op√©rationnelles n√©cessitant une validation externe.

